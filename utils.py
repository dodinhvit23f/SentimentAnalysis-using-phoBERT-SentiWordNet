import pdb

import pandas as pd
from tqdm import tqdm
tqdm.pandas()
import json
import numpy as np
import pickle
import os
import torch
from fairseq.data.encoders.fastbpe import fastBPE
from fairseq.data import Dictionary
import argparse
from vncorenlp import VnCoreNLP
import re
import string

VN_CHARS_LOWER = u'áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°'
VN_CHARS_UPPER = u'áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ'
VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER

replace_list = {
        'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©', 'á»e': 'oáº»',
        'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹', 'á»¥y': 'uá»µ', 'uáº£': 'á»§a',
        'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘', 'Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',
        'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»', 'ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',
        'eÌ‰': 'áº»', 'Ã k': u' Ã  ', 'aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯', 'Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',
        # Quy cÃ¡c icon vá» 2 loáº¡i emoj: TÃ­ch cá»±c hoáº·c tiÃªu cá»±c
        "ğŸ‘¹": "tá»‡", "ğŸ‘»": "tá»‘t", "ğŸ’ƒ": "tá»‘t", 'ğŸ¤™': ' tá»‘t ', 'ğŸ‘': ' tá»‘t ',
        "ğŸ’„": "tá»‘t", "ğŸ’": "tá»‘t", "ğŸ’©": "tá»‘t", "ğŸ˜•": "tá»‡", "ğŸ˜±": "tá»‡", "ğŸ˜¸": "tá»‘t",
        "ğŸ˜¾": "tá»‡", "ğŸš«": "tá»‡", "ğŸ¤¬": "tá»‡", "ğŸ§š": "tá»‘t", "ğŸ§¡": "tá»‘t", 'ğŸ¶': ' tá»‘t ',
        'ğŸ‘': ' tá»‡ ', 'ğŸ˜£': ' tá»‡ ', 'âœ¨': ' tá»‘t ', 'â£': ' tá»‘t ', 'â˜€': ' tá»‘t ',
        'â™¥': ' tá»‘t ', 'ğŸ¤©': ' tá»‘t ', 'like': ' tá»‘t ', 'ğŸ’Œ': ' tá»‘t ',
        'ğŸ¤£': ' tá»‘t ', 'ğŸ–¤': ' tá»‘t ', 'ğŸ¤¤': ' tá»‘t ', ':(': ' tá»‡ ', 'ğŸ˜¢': ' tá»‡ ',
        'â¤': ' tá»‘t ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜˜': ' tá»‘t ', 'ğŸ˜ª': ' tá»‡ ', 'ğŸ˜Š': ' tá»‘t ',
        '?': ' ? ', 'ğŸ˜': ' tá»‘t ', 'ğŸ’–': ' tá»‘t ', 'ğŸ˜Ÿ': ' tá»‡ ', 'ğŸ˜­': ' tá»‡ ',
        'ğŸ’¯': ' tá»‘t ', 'ğŸ’—': ' tá»‘t ', 'â™¡': ' tá»‘t ', 'ğŸ’œ': ' tá»‘t ', 'ğŸ¤—': ' tá»‘t ',
        '^^': ' tá»‘t ', 'ğŸ˜¨': ' tá»‡ ', 'â˜º': ' tá»‘t ', 'ğŸ’‹': ' tá»‘t ', 'ğŸ‘Œ': ' tá»‘t ',
        'ğŸ˜–': ' tá»‡ ', 'ğŸ˜€': ' tá»‘t ', ':((': ' tá»‡ ', 'ğŸ˜¡': ' tá»‡ ', 'ğŸ˜ ': ' tá»‡ ',
        'ğŸ˜’': ' tá»‡ ', 'ğŸ™‚': ' tá»‘t ', 'ğŸ˜': ' tá»‡ ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜„': ' tá»‘t ',
        'ğŸ˜™': ' tá»‘t ', 'ğŸ˜¤': ' tá»‡ ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜†': ' tá»‘t ', 'ğŸ’š': ' tá»‘t ',
        'âœŒ': ' tá»‘t ', 'ğŸ’•': ' tá»‘t ', 'ğŸ˜': ' tá»‡ ', 'ğŸ˜“': ' tá»‡ ', 'ï¸ğŸ†—ï¸': ' tá»‘t ',
        'ğŸ˜‰': ' tá»‘t ', 'ğŸ˜‚': ' tá»‘t ', ':v': '  tá»‘t ', '=))': '  tá»‘t ', 'ğŸ˜‹': ' tá»‘t ',
        'ğŸ’“': ' tá»‘t ', 'ğŸ˜': ' tá»‡ ', ':3': ' tá»‘t ', 'ğŸ˜«': ' tá»‡ ', 'ğŸ˜¥': ' tá»‡ ',
        'ğŸ˜ƒ': ' tá»‘t ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' tá»‘t ', 'ğŸ¤': ' tá»‘t ', 'ğŸˆ': ' tá»‘t ',
        'ğŸ˜—': ' tá»‘t ', 'ğŸ¤”': ' tá»‡ ', 'ğŸ˜‘': ' tá»‡ ', 'ğŸ”¥': ' tá»‡ ', 'ğŸ™': ' tá»‡ ',
        'ğŸ†—': ' tá»‘t ', 'ğŸ˜»': ' tá»‘t ', 'ğŸ’™': ' tá»‘t ', 'ğŸ’Ÿ': ' tá»‘t ',
        'ğŸ˜š': ' tá»‘t ', 'âŒ': ' tá»‡ ', 'ğŸ‘': ' tá»‘t ', ';)': ' tá»‘t ', '<3': ' tá»‘t ',
        'ğŸŒ': ' tá»‘t ', 'ğŸŒ·': ' tá»‘t ', 'ğŸŒ¸': ' tá»‘t ', 'ğŸŒº': ' tá»‘t ',
        'ğŸŒ¼': ' tá»‘t ', 'ğŸ“': ' tá»‘t ', 'ğŸ…': ' tá»‘t ', 'ğŸ¾': ' tá»‘t ', 'ğŸ‘‰': ' tá»‘t ',
        'ğŸ’': ' tá»‘t ', 'ğŸ’': ' tá»‘t ', 'ğŸ’¥': ' tá»‘t ', 'ğŸ’ª': ' tá»‘t ',
        'ğŸ’°': ' tá»‘t ', 'ğŸ˜‡': ' tá»‘t ', 'ğŸ˜›': ' tá»‘t ', 'ğŸ˜œ': ' tá»‘t ',
        'ğŸ™ƒ': ' tá»‘t ', 'ğŸ¤‘': ' tá»‘t ', 'ğŸ¤ª': ' tá»‘t ', 'â˜¹': ' tá»‡ ', 'ğŸ’€': ' tá»‡ ',
        'ğŸ˜”': ' tá»‡ ', 'ğŸ˜§': ' tá»‡ ', 'ğŸ˜©': ' tá»‡ ', 'ğŸ˜°': ' tá»‡ ', 'ğŸ˜³': ' tá»‡ ',
        'ğŸ˜µ': ' tá»‡ ', 'ğŸ˜¶': ' tá»‡ ', 'ğŸ™': ' tá»‡ ',
        # Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
        ':))': '  tá»‘t ', ':)': ' tá»‘t ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
        'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ': ' ok ', ' okay': ' ok ', 'okÃª': ' ok ',
        ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
        'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' tá»‘t ',
        'kg ': u' khÃ´ng ', 'not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '"k ': u' khÃ´ng ', ' kh ': u' khÃ´ng ',
        'kÃ´': u' khÃ´ng ', 'hok': u' khÃ´ng ', ' kp ': u' khÃ´ng pháº£i ', u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ',
        u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
        'he he': ' tá»‘t ', 'hehe': ' tá»‘t ', 'hihi': ' tá»‘t ', 'haha': ' tá»‘t ', 'hjhj': ' tá»‘t ',
        ' lol ': ' tá»‡ ', ' cc ': ' tá»‡ ', 'cute': u' dá»… thÆ°Æ¡ng ', 'huhu': ' tá»‡ ', ' vs ': u' vá»›i ',
        'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',
        ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',
        'Ä‘c': u' Ä‘Æ°á»£c ', 'authentic': u' chuáº©n chÃ­nh hÃ£ng ', u' aut ': u' chuáº©n chÃ­nh hÃ£ng ',
        u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' tá»‘t ', 'store': u' cá»­a hÃ ng ',
        'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ', 'god': u' tá»‘t ', 'wel done': ' tá»‘t ',
        'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
        'sáº¥u': u' xáº¥u ', 'gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t',
        'bt': u' bÃ¬nh thÆ°á»ng ',
        'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',
        'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng', 'chat': ' cháº¥t ', 'excelent': 'hoÃ n háº£o',
        'bad': 'tá»‡', 'fresh': ' tÆ°Æ¡i ', 'sad': ' tá»‡ ',
        'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ', 'quickly': u' nhanh ', 'quick': u' nhanh ',
        'fast': u' nhanh ', 'delivery': u' giao hÃ ng ', u' sÃ­p ': u' giao hÃ ng ',
        'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',
        u' order ': u' Ä‘áº·t hÃ ng ',
        'cháº¥t lg': u' cháº¥t lÆ°á»£ng ', u' sd ': u' sá»­ dá»¥ng ', u' dt ': u' Ä‘iá»‡n thoáº¡i ', u' nt ': u' nháº¯n tin ',
        u' tl ': u' tráº£ lá»i ', u' sÃ i ': u' xÃ i ', u'bjo': u' bao giá» ',
        'thik': u' thÃ­ch ', u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',
        u'quáº£ ng ': u' quáº£ng  ',
        'dep': u' Ä‘áº¹p ', u' xau ': u' xáº¥u ', 'delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ',
        'iu': u' yÃªu ', 'fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' tá»‘t ',
        ' por ': u' tá»‡ ', ' poor ': u' tá»‡ ', 'ib': u' nháº¯n tin ', 'rep': u' tráº£ lá»i ', u'fback': ' feedback ',
        'fedback': ' feedback ',
        # dÆ°á»›i 3* quy vá» 1*, trÃªn 3* quy vá» 5*
        '6 sao': ' 5 star ', '6 star': ' 5 star', '5star': ' 5star ', '5 sao': ' 5star ', '5sao': ' 5star ',
        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ', '2 sao': ' 1star ', '2sao': ' 1star ',
        '2 starstar': ' 1star ', '1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ', }

def no_marks(s):
    __INTAB = [ch for ch in VN_CHARS]
    __OUTTAB = "a"*17 + "o"*17 + "e"*11 + "u"*11 + "i"*5 + "y"*5 + "d"*2
    __OUTTAB += "A"*17 + "O"*17 + "E"*11 + "U"*11 + "I"*5 + "Y"*5 + "D"*2
    __r = re.compile("|".join(__INTAB))
    __replaces_dict = dict(zip(__INTAB, __OUTTAB))
    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)
    return result


def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1E0-\U0001F1FF"
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

def format_line(line):
    line = line.replace("\n","").strip()
    line = line.replace(u"\ufeff","") #"\ufeff"
    line = line.replace("\\", " ")

    line = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), line, flags=re.IGNORECASE)

    # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
    line = line.lower()

    # Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯


    for k, v in replace_list.items():
        line = line.replace(k, v)

    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    line = line.translate(translator)

    line = deEmojify(line)
    line = re.sub("\s+", " ", line)
    return line

def convert_lines(df, vocab, bpe, max_sequence_length):
    outputs = np.zeros((len(df), max_sequence_length))
    
    cls_id = 0
    eos_id = 2
    pad_id = 1

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        #pdb.set_trace()
        subwords = bpe.encode('<s> '+row.text+' </s>')
        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()
        if len(input_ids) > max_sequence_length: 
            input_ids = input_ids[:max_sequence_length] 
            input_ids[-1] = eos_id
        else:
            input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))
        outputs[idx,:] = np.array(input_ids)
    return outputs


def convert_lines_cnn(df, vocab, bpe, max_sequence_length, bert_leng = 256):
    outputs = np.zeros((len(df), max_sequence_length))

    cls_id = 0
    eos_id = 2
    pad_id = 1

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        # pdb.set_trace()
        subwords = bpe.encode('<s> ' + format_line(row.text) + ' </s>')
        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()
        if len(input_ids) > bert_leng:
            input_ids[256 - 1] = eos_id
        else:
            input_ids = input_ids + [pad_id, ] * (max_sequence_length - len(input_ids))

        if( len(input_ids) < max_sequence_length):
            input_ids = input_ids +[pad_id, ] * (max_sequence_length - len(input_ids))

        #print(len(input_ids))
        outputs[idx, :] = np.array(input_ids)
    return outputs



def convert_lines_with_Viet_setiwordnet(df, vocab, bpe, max_sequence_length):

    outputs = np.zeros((len(df), max_sequence_length))
    """
    pathToSentiFile = "./VietSentiWordnet/VietSentiWordnet_Ver1.3.5.txt"

    load setiwordnetFile

    Senti = dict({})
    f = open(pathToSentiFile, 'r', encoding="utf-8")
    first = True
    for i in f:
        if i.strip() == "":
            continue

        if (first):
            first = False
            continue

        ws = i.split('\t')
        key = ws[4].replace("#", "")
        for i in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:
            key = key.replace(i, "")
        # print(ws[2])
        pos = float(ws[2])
        neg = float(ws[3])
        if key not in Senti:
            Senti.update({key: [pos, neg]})
    f.close()
    """
    cls_id = 0
    eos_id = 2
    pad_id = 1

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        #pdb.set_trace()
        value = " tÃ­ch_cá»±c"
        if(row.label == 1):
            value = " tiÃªu_cá»±c"

        subwords = bpe.encode('<s> ' + row.text + value + ' </s>')
        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()

        if len(input_ids) > max_sequence_length:
            input_ids = input_ids[:max_sequence_length]
            input_ids[-1] = eos_id
        else:
            input_ids = input_ids + [pad_id, ] * (max_sequence_length - len(input_ids))

        outputs[idx, :] = np.array(input_ids)
    return outputs

def seed_everything(SEED):
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True

def GetVec (SentiDirtional ,string):
    global Senti
    pos, neg, c=0,0, 0
    for w in string.split():
        if w in Senti:
            pos = pos+ Senti[w][0]
            neg = pos+ Senti[w][1]
    pos= pos/len(string)  # pos= pos/len(s) pos= pos/c
    neg= neg/len(string)  #neg = neg/len(s) neg= neg/c
    return pos, neg

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
"""
if __name__ == '__main__':
    # Load the dictionary
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--train_path', type=str, default='./data/train.csv')
    parser.add_argument('--dict_path', type=str, default="./phobert/dict.txt")
    parser.add_argument('--config_path', type=str, default="./phobert/config.json")
    parser.add_argument('--rdrsegmenter_path', type=str, required=True)
    parser.add_argument('--pretrained_path', type=str, default='./phobert/model.bin')
    parser.add_argument('--max_sequence_length', type=int, default=256)
    parser.add_argument('--batch_size', type=int, default=24)
    parser.add_argument('--accumulation_steps', type=int, default=5)
    parser.add_argument('--epochs', type=int, default=5)
    parser.add_argument('--fold', type=int, default=0)
    parser.add_argument('--seed', type=int, default=69)
    parser.add_argument('--lr', type=float, default=3e-5)
    parser.add_argument('--ckpt_path', type=str, default='./models')
    parser.add_argument('--bpe-codes', default="./phobert/bpe.codes", type=str, help='path to fastBPE BPE')

    args = parser.parse_args()
    bpe = fastBPE(args)
    rdrsegmenter = VnCoreNLP(args.rdrsegmenter_path, annotators="wseg", max_heap_size='-Xmx500m')

    seed_everything(69)

    vocab = Dictionary()
    vocab.add_from_file(args.dict_path)

    # Load training data
    train_df = pd.read_csv(args.train_path, sep='\t').fillna("###")
    
    Chuáº©n hÃ³a láº¡i chuá»—i dÃ¹ng tokenize sáº£n pháº©m -> sáº£n_pháº©m 
    
    train_df.text = train_df.text.progress_apply(
        lambda x: ' '.join([' '.join(sent) for sent in rdrsegmenter.tokenize(x)]))
    y = train_df.label.values
    
    Tráº£ vá» ma tráº­n vector embedding [dÃ²ng, 256]
    
    X_train = convert_lines_with_Viet_setiwordnet(train_df, vocab, bpe, args.max_sequence_length)
"""